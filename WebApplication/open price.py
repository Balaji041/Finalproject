# -*- coding: utf-8 -*-
"""CNN .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-N5TERL1wB8NvQvyNWSTVYF1aXjAv7_H
"""

import numpy as np # linear algebra
import pandas as pd

df = pd.read_csv('/content/TSLA (1).csv')

df.head(3)

df.shape

df = df.drop(columns=['Date'])

df.head()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, accuracy_score

plt.figure(figsize=(15,15))
plt.plot(df['Open'])

scaler = MinMaxScaler(feature_range=(0,1))
df = scaler.fit_transform(df)   

y_open = df[:,0]
y_open
temp=[]
for i in df:
  i=i[1:]
  temp.append(i)
df=temp
print(len(df[0]))
df[0]

plt.figure(figsize=(15,15))
plt.plot(y_open)
plt.show()

#splitting data set into 80 for testing
ntrain = int(len(y_open)*0.8) 

train = df[0:ntrain]
test  = df[ntrain:len(df)]
print(train)
y_open_train = y_open[0:ntrain]
y_open_test  = y_open[ntrain:len(y_open)]

y_open_test.shape

import numpy as np

def to_sequences(seq_size, data,open):
    x = []
    y = []
    #we have mapped for 10 days duration to increase the accuracy of model
    #10 sets of x class and y sets of y class were mapped as a single unit object
    #instead of training model over single instace i.e for single day data
    #we have trained for 10 different instance treating them as single unit
    for i in range(len(data)-seq_size-1):
        window = data[i:(i+seq_size)]
        after_window = open[i+seq_size]
        window = [[x] for x in window]
        x.append(window)
        y.append(after_window)
        
    return np.array(x),np.array(y)


timesteps = 10

x_train, y_train = to_sequences(timesteps, train, y_open_train)
x_test, y_test   = to_sequences(timesteps, test, y_open_test)

print("Shape of x_train: {}".format(x_train.shape))
print("Shape of x_test: {}".format(x_test.shape))
print("Shape of y_train: {}".format(y_train.shape))
print("Shape of y_test: {}".format(y_test.shape))

x_train[0]

x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[2], x_train.shape[1],x_train.shape[3]))
x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[2],x_test.shape[1],x_test.shape[3]))

print(x_train.shape)

x_train[0][0].shape

fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5)
ax1.imshow(x_train[0][0])
ax2.imshow(x_train[1][0])
ax3.imshow(x_train[2][0])
ax4.imshow(x_train[3][0])
ax5.imshow(x_train[4][0])

import numpy as np
import pandas as pd

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten, Dropout
from keras.optimizers import Adam
from keras.layers import Conv1D, Conv2D, MaxPooling2D
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.preprocessing import MinMaxScaler
import csv
import collections
from scipy.stats import zscore
from datetime import datetime
import matplotlib.pyplot as plt

cnn = Sequential()
cnn.add(Conv2D(32, kernel_size = (1, 2), strides = (1, 1),  padding = 'valid', 
               activation = 'relu', input_shape = (1,10,5)))
cnn.add(MaxPooling2D(pool_size = (1,2)))

cnn.add(Flatten())
cnn.add(Dense(64, activation="relu"))
cnn.add(Dropout(0.5))
cnn.add(Dense(1, activation="relu"))
cnn.summary()

cnn.compile(loss='mean_squared_error', optimizer='adam')

monitor = EarlyStopping(monitor='val_loss', min_delta=1, patience=2, verbose=2, mode='auto') 
checkpointer = ModelCheckpoint(filepath="CNN", verbose=0, save_best_only=True) # save best model

history = cnn.fit(tf.expand_dims(x_train,axis=-1),y_train,validation_split=0.2,batch_size = 128, callbacks=[checkpointer],epochs = 20)

plt.plot(history.history['loss'], label = 'loss')
plt.plot(history.history['val_loss'], label = 'val loss')
plt.legend()

cnn.load_weights('CNN.hdf5')

pred = cnn.predict(x_test)
print(pred.shape)

score = np.sqrt(metrics.mean_squared_error(y_test, pred))
print("RMSE Score: {}".format(score))

plt.figure(figsize=(15,15))

plt.plot(y_test, label = 'actual')
plt.plot(pred,   label = 'predicted')


plt.legend()
plt.show()

from sklearn import metrics
print("Mean Absolute Error:", round(metrics.mean_absolute_error(y_test, pred), 4))
print("Mean Squared Error:", round(metrics.mean_squared_error(y_test, pred), 4))
print("Root Mean Squared Error:", round(np.sqrt(metrics.mean_squared_error(y_test, pred)), 4))
print("(R^2) Score:", round(metrics.r2_score(y_test, pred), 5))
#print(f'Train Score : {model.score(X_train, y_train) * 100:.2f}% and Test Score : {model.score(X_test, y_test) * 100:.2f}% using Random Tree Regressor.')
errors = abs(pred - y_test)
mape = 100 * (errors / y_test)
accuracy1 = 100 - np.mean(mape)
print('Accuracy:', round(accuracy1, 2), '%.')

# -*- coding: utf-8 -*-
"""CNN sent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xvhO0P8OIq2ELbwDmcmVTXNv_-9CmwGJ
"""

import numpy as np # linear algebra
import pandas as pd 
df=pd.read_csv('/content/StockMarketTweets2122.csv')
df.head()

x=df['sentiment']
y = []
for i in x:
  if i == 'Positive':
    y.append(1)
  elif i == 'Negative' or i == 'Neutral':
    y.append(0)
df['sentiment'] = y

# del data['0']
df

y=df['sentiment']



from sklearn.model_selection import train_test_split
df_train,df_test,y_train,y_test=train_test_split(df['Tweet'],y,test_size=0.33,random_state=42)
print('DF Train Shape: ',df_train.shape)
print('DF Test Shape: ',df_test.shape)
print('Y Train Shape: ',y_train.shape)
print('Y Test Shape: ',y_test.shape)

from tensorflow.keras.preprocessing.text import Tokenizer
max_words=10000
tokenizer=Tokenizer(max_words)
tokenizer.fit_on_texts(df_train)
sequence_train=tokenizer.texts_to_sequences(df_train)
sequence_test=tokenizer.texts_to_sequences(df_test)

word2vec=tokenizer.word_index
V=len(word2vec)
print('dataset has %s number of independent tokens' %V)

from tensorflow.keras.preprocessing.sequence import pad_sequences
data_train=pad_sequences(sequence_train)
data_train.shape

T=data_train.shape[1]
data_test=pad_sequences(sequence_test,maxlen=T)
data_test.shape

from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding
from tensorflow.keras.models import Model

D=20
i=Input((T,))
x=Embedding(V+1,D)(i)
x=Conv1D(32,3,activation='relu')(x)
x=MaxPooling1D(3)(x)
x=Conv1D(64,3,activation='relu')(x)
x=MaxPooling1D(3)(x)
x=Conv1D(128,3,activation='relu')(x)
x=GlobalMaxPooling1D()(x)
x=Dense(5,activation='softmax')(x)
model=Model(i,x)
model.summary()

model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
cnn_senti=model.fit(data_train,y_train,validation_data=(data_test,y_test),epochs=50,batch_size=100)

y_pred=model.predict(data_test)
y_pred

y_pred=np.argmax(y_pred,axis=1)
y_pred

from sklearn.metrics import confusion_matrix,classification_report
import seaborn as sns

cm=confusion_matrix(y_test,y_pred)
ax=sns.heatmap(cm,annot=True,cmap='Blues',fmt=' ')
ax.set_title('Confusion Matrix')
ax.set_xlabel('y_test')
ax.set_ylabel('y_pred')

print(classification_report(y_test,y_pred))

from sklearn.metrics import accuracy_score
Accuracy2=(metrics.accuracy_score(y_test,y_pred))
Accuracy2=Accuracy2*100
print(Accuracy2)

Accuracy=accuracy1*0.8+Accuracy2*0.2
print(Accuracy)